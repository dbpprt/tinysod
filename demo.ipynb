{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dbpprt/miniforge3/envs/tinysod/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from modeling.tiny_vit_sam import TinyViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TinyViT(\n",
    "    img_size=1024,\n",
    "    in_chans=3,\n",
    "    num_classes=1000,\n",
    "    embed_dims=[64, 128, 160, 320],\n",
    "    depths=[2, 2, 6, 2],\n",
    "    num_heads=[2, 4, 5, 10],\n",
    "    window_sizes=[7, 7, 14, 7],\n",
    "    mlp_ratio=4.0,\n",
    "    drop_rate=0.0,\n",
    "    drop_path_rate=0.0,\n",
    "    use_checkpoint=False,\n",
    "    mbconv_expand_ratio=4.0,\n",
    "    local_conv_size=3,\n",
    "    layer_lr_decay=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinysam = torch.load(\"./checkpoints/tinysam.pth\", map_location=\"cpu\")\n",
    "encoder_keys = [key for key in tinysam.keys() if key.startswith(\"image_encoder\")]\n",
    "encoder_weights = {key[len(\"image_encoder.\") :]: tinysam[key] for key in encoder_keys}\n",
    "encoder.load_state_dict(encoder_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms.functional import resize, to_pil_image  # type: ignore\n",
    "\n",
    "\n",
    "class ResizeLongestSide:\n",
    "    \"\"\"\n",
    "    Resizes images to the longest side 'target_length', as well as provides\n",
    "    methods for resizing coordinates and boxes. Provides methods for\n",
    "    transforming both numpy array and batched torch tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_length: int) -> None:\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def apply_image(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array with shape HxWxC in uint8 format.\n",
    "        \"\"\"\n",
    "        target_size = self.get_preprocess_shape(image.shape[0], image.shape[1], self.target_length)\n",
    "        return np.array(resize(to_pil_image(image), target_size))\n",
    "\n",
    "    def apply_coords(self, coords: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array of length 2 in the final dimension. Requires the\n",
    "        original image size in (H, W) format.\n",
    "        \"\"\"\n",
    "        old_h, old_w = original_size\n",
    "        new_h, new_w = self.get_preprocess_shape(original_size[0], original_size[1], self.target_length)\n",
    "        coords = deepcopy(coords).astype(float)\n",
    "        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n",
    "        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n",
    "        return coords\n",
    "\n",
    "    def apply_boxes(self, boxes: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array shape Bx4. Requires the original image size\n",
    "        in (H, W) format.\n",
    "        \"\"\"\n",
    "        boxes = self.apply_coords(boxes.reshape(-1, 2, 2), original_size)\n",
    "        return boxes.reshape(-1, 4)\n",
    "\n",
    "    def apply_image_torch(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expects batched images with shape BxCxHxW and float format. This\n",
    "        transformation may not exactly match apply_image. apply_image is\n",
    "        the transformation expected by the model.\n",
    "        \"\"\"\n",
    "        # Expects an image in BCHW format. May not exactly match apply_image.\n",
    "        target_size = self.get_preprocess_shape(image.shape[2], image.shape[3], self.target_length)\n",
    "        return F.interpolate(image, target_size, mode=\"bilinear\", align_corners=False, antialias=True)\n",
    "\n",
    "    def apply_coords_torch(self, coords: torch.Tensor, original_size: Tuple[int, ...]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expects a torch tensor with length 2 in the last dimension. Requires the\n",
    "        original image size in (H, W) format.\n",
    "        \"\"\"\n",
    "        old_h, old_w = original_size\n",
    "        new_h, new_w = self.get_preprocess_shape(original_size[0], original_size[1], self.target_length)\n",
    "        coords = deepcopy(coords).to(torch.float)\n",
    "        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n",
    "        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n",
    "        return coords\n",
    "\n",
    "    def apply_boxes_torch(self, boxes: torch.Tensor, original_size: Tuple[int, ...]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expects a torch tensor with shape Bx4. Requires the original image\n",
    "        size in (H, W) format.\n",
    "        \"\"\"\n",
    "        boxes = self.apply_coords_torch(boxes.reshape(-1, 2, 2), original_size)\n",
    "        return boxes.reshape(-1, 4)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_preprocess_shape(oldh: int, oldw: int, long_side_length: int) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Compute the output size given input size and target long side length.\n",
    "        \"\"\"\n",
    "        scale = long_side_length * 1.0 / max(oldh, oldw)\n",
    "        newh, neww = oldh * scale, oldw * scale\n",
    "        neww = int(neww + 0.5)\n",
    "        newh = int(newh + 0.5)\n",
    "        return (newh, neww)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "pixel_mean = [123.675, 116.28, 103.53]\n",
    "pixel_mean = torch.Tensor(pixel_mean).view(-1, 1, 1)\n",
    "pixel_std = [58.395, 57.12, 57.375]\n",
    "pixel_std = torch.Tensor(pixel_std).view(-1, 1, 1)\n",
    "\n",
    "\n",
    "def preprocess(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Normalize pixel values and pad to a square input.\"\"\"\n",
    "    # Normalize colors\n",
    "    x = (x - pixel_mean) / pixel_std\n",
    "\n",
    "    # Pad\n",
    "    h, w = x.shape[-2:]\n",
    "    padh = 1024 - h\n",
    "    padw = 1024 - w\n",
    "    x = F.pad(x, (0, padw, 0, padh))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "image = cv2.imread(\"hanna.jpeg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "transform = ResizeLongestSide(1024)\n",
    "image = transform.apply_image(image)\n",
    "image = torch.as_tensor(image)\n",
    "image = image.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "image = preprocess(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1024, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = encoder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation.deeplabv3 import deeplabv3_mobilenet_v3_large\n",
    "\n",
    "backbone = deeplabv3_mobilenet_v3_large(num_classes=1)\n",
    "x = backbone.backbone(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 960, 64, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"out\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 64, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
    "\n",
    "head = DeepLabHead(in_channels=4, num_classes=1)\n",
    "head.eval()\n",
    "head(features.view(1, 4, 512, 512)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024, 1024])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"Very simple multi-layer perceptron (also called FFN)\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        h = [hidden_dim] * (num_layers - 1)\n",
    "        self.layers = nn.ModuleList(\n",
    "            nn.Linear(n, k) for n, k in zip([input_dim] + h, h + [output_dim])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x)) if i < self.num_layers - 1 else layer(x)\n",
    "        return x\n",
    "    \n",
    "head = nn.Sequential(*[\n",
    "    nn.Conv2d(1, 64, 3, padding=1, bias=False),\n",
    "    nn.BatchNorm2d(64),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Conv2d(64, 1, 1),\n",
    "])\n",
    "\n",
    "head = MLP(1024, 64, 1024, 8)\n",
    "head(features.view(1, 1, 1024, 1024)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157120"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "sum([np.prod(p.size()) for p in head.parameters()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinysod",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
